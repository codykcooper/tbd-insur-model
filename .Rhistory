names()
bad_var_perzero =  dt %>%
select_if(percent_zero) %>%
names()
bad_var_1percent
bad_var_perzero
bad_var_perzero =  dt %>%
select_if(percent_zero) %>%
names()
bad_var = c(bad_var_1percent,bad_var_perzero) %>%
unique()
bad_var
#Remove highly correlated / redundent variables
iv = setdiff(dv)
dt[, (bad_var) := NULL]
#Remove highly correlated / redundent variables
iv = setdiff(dv,c('loan_convert','enc_dti_bottom_ratio_percent'))
#Remove highly correlated / redundent variables
iv = setdiff(names(dt),c('loan_convert','enc_dti_bottom_ratio_percent'))
iv
View(iv)
cor_tab = cor(dt[,..iv])
cor_tab
cor_tab = cor(dt[,..iv], use = 'pairwise.complete.obs')
cor_tab
high_cor = findCorrelation(cor_tab, cutoff = .7, exact = T)
high_cor = findCorrelation(cor_tab, cutoff = .7, exact = T, names = T)
View(cor_tab)
cor_tab = cor(dt[,..iv])
cor_tab
high_cor = findCorrelation(cor_tab, cutoff = .7, exact = T, names = T)
cor_tab = cor(dt[,..iv], use = 'pairwise.complete.obs')
cor_tab
idx = apply(cor_tab, 1, function(x) sum(is.na(x)))
idx
table(idc)
table(idx)
nrow(dt[complete.cases(dt)])
cor_tab = cor(dt[,..iv], use = 'pairwise.complete.obs')
idx = apply(cor_tab, 1, function(x) sum(is.na(x)))
table(idx)
names(which(idx>0))
table(dt[["enc_loan_program_name_Conventional - RateTerm Refinance - Fixed"]])
608/nrow(dt)
library(data.table)
library(bit64)
library(tidyverse)
library(lubridate)
library(openxlsx)
source('/Users/codycooper/Documents/Lower/Segmentation/Syntax/Support Functions.R')
dt = fread('/Users/codycooper/Documents/Lower/Segmentation/Data/Raw/cody_loan_data_201905281036.csv')
sapply(dt, class) %>%
unlist() %>%
table()
bad_var = dt %>%
select_if(percent_1value) %>%
names()
bad_var = c(bad_var,
grep('loan_number|zip|city|state|county',names(dt), value=T),
"enc_current_status","enc_loan_milestone_current_name","enc_loan_folder") #Removing Geo information for now
#Delete bad vars
dt[, (bad_var) := NULL]
factor_var = c(
'enc_loan_amor_term_months',
'enc_cost_center'
)
numeric_var = dt %>%
select_if(is.numeric) %>%
names() %>%
setdiff(c(factor_var,"enc_current_status",'enc_loan_folder_My Pipeline','enc_loan_milestone_current_name_Disclosure Sent',
'enc_loan_folder_Closed','enc_current_status_Loan Originated','enc_loan_milestone_current_name_Purchased',
'enc_current_status_Active Loan'))
#Check Percent Zero on vars
dt[, lapply(.SD, percent_missing), .SD = names(dt)]
#Need to create binaries (0 vs not zero)
zero.95 <- dt %>%
select_if(percent_zero) %>%
names()
rm(list = ls())
library(data.table)
library(h2o)
library(caret)
source('/Users/codycooper/Documents/Lower/Segmentation/Syntax/Support Functions.R')
most_recent = file.info(list.files("/Users/codycooper/Documents/Lower/Segmentation/Data/Model Data", full.names = T)) %>%
as.data.table(keep.rownames = T) %>%
filter(mtime == max(mtime)) %>%
select(rn)
dt = fread(most_recent$rn)
#Ignore binary
bin_vars = dt %>%
select_if(is.binary) %>%
names()
dfs_vars = setdiff(names(dt), c(bin_vars,'loan_convert','enc_dti_bottom_ratio_percent'))
dt[, lapply(.SD, percent_missing), .SD = dfs_vars]
#Coborrower_age has too many missing
dfs_vars = setdiff(dfs_vars, 'coborrower_age')
divide_vars(dt, dfs_vars)
names(dt)
#count infinite values
dt[, lapply(.SD, function(x) sum(is.infinite(x))), .SD = names(dt)] %>% max()
bad_var_1percent = dt %>%
select_if(percent_1value) %>%
names()
bad_var_perzero =  dt %>%
select_if(percent_zero) %>%
names()
bad_var_1percent
bad_var_perzero
bad_var = c(bad_var_1percent,bad_var_perzero) %>%
unique()
dt[, (bad_var) := NULL]
#Remove highly correlated / redundent variables
iv = setdiff(names(dt),c('loan_convert','enc_dti_bottom_ratio_percent'))
cor_tab = cor(dt[,..iv], use = 'pairwise.complete.obs')
idx = apply(cor_tab, 1, function(x) sum(is.na(x)))
names(which(idx>0))
summary(dt[[enc_rate_div_enc_rate_undiscounted"]])
summary(dt[["enc_rate_div_enc_rate_undiscounted"]])
3494/nrow(dt)
percent_1value
bad_var_1percent = dt %>%
select_if(percent_1value, crt = .85) %>%
names()
bad_var_1percent
x = dt[["enc_rate_div_enc_rate_undiscounted"]]
x
crt =.85
max(table(x)/sum(table(x)),na.rm = T)
max(sum(is.na(x))/length(x)
)
max(
max(table(x)/sum(table(x)),na.rm = T),
max(sum(is.na(x))/length(x))
,na.rm=T
) > crt
bad_var_1percent = dt %>%
select_if(percent_1value, crt = .85) %>%
names()
bad_var_1percent
bad_var_1percent
bad_var_1percent
# Remove anything with low variability
percent_1value <- function(x,crt = .85){
suppressWarnings(
max(
max(table(x)/sum(table(x)),na.rm = T),
max(sum(is.na(x))/length(x))
,na.rm=T
) > crt
)
}
bad_var_1percent = dt %>%
select_if(percent_1value) %>%
names()
bad_var_1percent
summary(dt$enc_down_payment_amount)
table(dt$enc_down_payment_amount)
# Remove anything with low variability
percent_1value <- function(x,crt = .9){
suppressWarnings(
max(
max(table(x)/sum(table(x)),na.rm = T),
max(sum(is.na(x))/length(x))
,na.rm=T
) > crt
)
}
bad_var_1percent = dt %>%
select_if(percent_1value) %>%
names()
bad_var_1percent
summary(dt[["enc_sec_sub_lien_amount_div_total_asset" ]])
x = (dt[["enc_sec_sub_lien_amount_div_total_asset" ]])
max(table(x)/sum(table(x)),na.rm = T)
max(sum(is.na(x))/length(x))
table(x)
table(x)/sum(table(x))
table(x)
1331/nrow(x)
table(x)/sum(table(x, useNA = 'ifany')
)
max(table(x)/sum(table(x, useNA = 'ifany')),na.rm = T)
# Remove anything with low variability
percent_1value <- function(x,crt = .9){
suppressWarnings(
max(
max(table(x)/sum(table(x, useNA = 'ifany')),na.rm = T),
max(sum(is.na(x))/length(x))
,na.rm=T
) > crt
)
}
bad_var_1percent = dt %>%
select_if(percent_1value) %>%
names()
bad_var_1percent
summary(dt[["enc_sec_sub_lien_amount_div_borrower_age" ]])
bad_var_perzero =  dt %>%
select_if(percent_zero) %>%
names()
bad_var_perzero
bad_var_1percent
bad_var = c(bad_var_1percent,bad_var_perzero) %>%
unique()
dt[, (bad_var) := NULL]
#Remove highly correlated / redundent variables
iv = setdiff(names(dt),c('loan_convert','enc_dti_bottom_ratio_percent'))
iv
cor_tab = cor(dt[,..iv], use = 'pairwise.complete.obs')
idx = apply(cor_tab, 1, function(x) sum(is.na(x)))
names(which(idx>0))
high_cor = findCorrelation(cor_tab, cutoff = .7, exact = T, names = T)
high_cor
setdiff(iv, high_cor)
high_cor = findCorrelation(cor_tab, cutoff = .75, exact = T, names = T)
high_cor
keep_var = setdiff(iv, high_cor)
keep_var
fwrite(dt[c('enc_dti_bottom_ratio_percent',keep_var)],'/Users/codycooper/Documents/Lower/Segmentation/Data/Data with New Features.csv')
fwrite(dt[c('enc_dti_bottom_ratio_percent',keep_var), with = F],'/Users/codycooper/Documents/Lower/Segmentation/Data/Data with New Features.csv')
fwrite(dt[,c('enc_dti_bottom_ratio_percent',keep_var), with = F],'/Users/codycooper/Documents/Lower/Segmentation/Data/Data with New Features.csv')
rm(list = ls())
library(data.table)
library(h2o)
library(tidyverse)
source('/Users/codycooper/Documents/Lower/Segmentation/Syntax/Support Functions.R')
options(stringsAsFactors = F)
dt = fread('/Users/codycooper/Documents/Lower/Segmentation/Data/Data with New Features.csv')
dv = 'enc_dti_bottom_ratio_percent'
#Remove anything derived from the dv
remove_var = grep(paste(c(dv,'enc_dti_top_ratio_percent'),collapse = '|'),names(dt), value = T)
remove_var
rm(list = ls())
library(data.table)
library(h2o)
library(caret)
source('/Users/codycooper/Documents/Lower/Segmentation/Syntax/Support Functions.R')
most_recent = file.info(list.files("/Users/codycooper/Documents/Lower/Segmentation/Data/Model Data", full.names = T)) %>%
as.data.table(keep.rownames = T) %>%
filter(mtime == max(mtime)) %>%
select(rn)
dt = fread(most_recent$rn)
#Ignore binary
bin_vars = dt %>%
select_if(is.binary) %>%
names()
dfs_vars = setdiff(names(dt), c(bin_vars,'loan_convert','enc_dti_bottom_ratio_percent'))
dfs_vars
dt[, lapply(.SD, percent_missing), .SD = dfs_vars]
#Coborrower_age has too many missing
dfs_vars = setdiff(dfs_vars, 'coborrower_age')
divide_vars(dt, dfs_vars)
dfs_vars
library(data.table)
library(h2o)
library(tidyverse)
source('/Users/codycooper/Documents/Lower/Segmentation/Syntax/Support Functions.R')
options(stringsAsFactors = F)
dt = fread('/Users/codycooper/Documents/Lower/Segmentation/Data/Data with New Features.csv')
dv = 'enc_dti_bottom_ratio_percent'
#Remove anything derived from the dv
remove_var = grep(paste(c(dv,'enc_dti_top_ratio_percent'),collapse = '|'),names(dt), value = T)
remove_var = c(remove_var,dt[, lapply(.SD, percent_missing), .SD = names(dt)] %>%
melt() %>%
.[value>.75,as.character(variable)],
'loan_convert','enc_total_monthly_payment_amount')
remove_var
#Remove anything derived from the dv
remove_var = grep(paste(c(dv,'enc_dti_top_ratio_percent'),collapse = '|'),names(dt), value = T)
remove_var = c(remove_var,dt[, lapply(.SD, percent_missing), .SD = names(dt)] %>%
melt() %>%
.[value>.75,as.character(variable)],
'loan_convert','enc_total_monthly_payment_amount')
remove_var
X_var = setdiff(names(dt), remove_var)
X_var
h2o.init(max_mem_size = "12g")
dt_h2o = as.h2o(dt)
#Set seed for reproducibility
seed_set = 43125
# Some XGboost/GBM hyperparameters
hyper_params <- list(ntrees = seq(20, 100, 2),
learn_rate = seq(0.1, 0.5, 0.01),
max_depth = seq(1, 8, 1),
sample_rate = seq(0.75, 1, 0.01),
col_sample_rate = seq(0.25, .75, 0.01))
search_criteria <- list(strategy = "RandomDiscrete",
max_models = 100,
seed = seed_set)
#Standardize variable
# Train the grid
xgb_grid <- h2o.grid(algorithm = "xgboost",
x = X_var, y = dv,
training_frame = dt_h2o,
nfolds = 5,
hyper_params = hyper_params,
search_criteria = search_criteria)
# Sort the grid by CV AUC
grid <- h2o.getGrid(grid_id = xgb_grid@grid_id,sort_by = 'r2', decreasing = TRUE);grid
# Sort the grid by CV AUC
grid <- h2o.getGrid(grid_id = xgb_grid@grid_id,sort_by = 'rmse', decreasing = TRUE);grid
# Sort the grid by CV AUC
grid <- h2o.getGrid(grid_id = xgb_grid@grid_id,sort_by = 'r2', decreasing = TRUE);grid
grid_top_model <- grid@summary_table[1, "model_ids"] %>%
h2o.getModel()
VI = h2o.varimp(grid_top_model)
View(VI)
VI = h2o.varimp(grid_top_model) %>%
as.data.table()
names(VI)
VI[scaled_importance > .01,variable]
VI[scaled_importance > .1,variable]
#select top 50 as initiatil candidates for clustering
clust_cand = VI[scaled_importance > .1,variable]
clust_cand
#Remove highly correlated variables
cor_mat = cor(dt[,..clust_cand], use = 'pairwise.complete.obs')
cor_mat
library(caret)
redundent_features = findCorrelation(cor_mat, cutoff = .65, names = T)
redundent_features
redundent_features = findCorrelation(cor_mat, cutoff = .75, names = T)
redundent_features
clust_cand = setdiff(clust_cand, redundent_features)
clust_cand
redundent_features = findCorrelation(cor_mat, cutoff = .7, names = T)
redundent_features
clust_cand = setdiff(clust_cand, redundent_features)
clust_cand
bin_name = dt[, clust_cand, with = F] %>%
select_if(is.binary) %>%
names()
bin_name
table(dtt[["enc_loan_amor_type_Adjust"]])
table(dt[["enc_loan_amor_type_Adjust"]])
bin_idx = which(clust_cand %in% bin_name)
bin_idx
## Get normalization
library(bestNormalize)
bn = dt[,lapply(.SD,bestNormalize), .SD = clust_cand]
warnings()
View(bn)
bn
bn = dt[,lapply(.SD,function(x) bestNormalize(x)$x.t), .SD = clust_cand]
View(bn)
#mean replace due to missing
bn[, lapply(.SD, function(x) sum(is.na(x))), .SD = names(bn)]
dt[, lapply(.SD, function(x) sum(is.na(x))), .SD = clust_cand]
2029/nrow(Dt)
2029/nrow(dt)
clust_cand = setdiff(clust_cand, 'enc_borr_coborr_income_per_month_div_enc_heloc_amount')
clust_cand
bn = dt[,lapply(.SD,function(x) bestNormalize(x)$x.t), .SD = clust_cand]
#mean replace due to missing
bn[, lapply(.SD, function(x) sum(is.na(x))), .SD = names(bn)]
dt[, lapply(.SD, function(x) sum(is.na(x))), .SD = clust_cand]
bn = bn[complete.cases(bn)]
c6 = kmeans(bn, 6)
table(c6$centers)
table(c6$cluster)
names(dt)
dt$c6 = c6$cluster
cluster
c6$cluster
h2o.clusterInfo()
#Generate model
dt_model = dt[, clust_cand, with = F]
dt_model[complete.cases(dt_model)]
dt_model = dt_model[complete.cases(dt_model)]
dt_model$c6 = c6$cluster
h2o_cmod = as.h2o(dt_model)
aml = h2o.automl(
x = setdiff(names(dt_model), 'c6'),
y = 'c6',
training_frame = h2o_cmod,
max_models = 50
)
aml@leader
class(dt_model$c6)
dt_model$c6 = as.factor(c6$cluster)
aml@leader %>% h2o.confusionMatrix()
dt_model$c6
h2o_cmod = as.h2o(dt_model)
aml = h2o.automl(
x = setdiff(names(dt_model), 'c6'),
y = 'c6',
training_frame = h2o_cmod,
max_models = 50
)
h2o.confusionMatrix(aml@leader)
aml@leader
aml
pred_6 = h2o.predict(aml@leader, as.h2o(dt))
pred_6
table(pred_6$predict)
table(pred_6[,1])
pred_6[,1]
table(as.data.frame(pred_6)$predict)
dt$c6 = as.data.frame(pred_6)$predict
dt[, lapply(.SD, mean, na.rm = T), .SD = names(dt)[1:6], by = 'c6']
dt[, lapply(.SD, mean, na.rm = T), .SD = names(dt)[1:6], by = 'c6'] %>% t()
dt[, lapply(.SD, mean, na.rm = T), .SD = names(dt)[1:6], by = 'c6'] %>% View()
dt[, lapply(.SD, mean, na.rm = T), .SD = unique(c(names(dt)[1:6],clust_cand)), by = 'c6'] %>% View()
summary(dt$enc_ltv_div_enc_total_monthly_payment_amount)
hist(dt$enc_ltv_div_enc_total_monthly_payment_amount)
dt = fread('/Users/codycooper/Documents/Lower/Segmentation/Data/Raw/cody_loan_data_201905281036.csv')
dt$enc_ltv %>% summary()
dt$enc_total_monthly_payment_amount %>% summary()
summary(dt[, enc_ltv/enc_total_monthly_payment_amount])
dt = fread('/Users/codycooper/Documents/Lower/Segmentation/Data/Raw/cody_loan_data_201905281036.csv')
dt = fread('/Users/codycooper/Documents/Lower/Segmentation/Data/Raw/cody_loan_data_201905281036.csv')
dt[, .N, by = 'zip'] %>% View()
dt = fread('/Users/codycooper/Documents/Lower/Segmentation/Data/Raw/cody_loan_data_201905281036.csv')
View(names(dt))
dt[, .N, by = 'enc_subject_property_zip'] %>% View()
library(data.table)
dt = fread('/Users/codycooper/Documents/GitHub/dbt-example/data/pre-existing-conditions-by-congressional-district.xlsx - number with pre-ex by state.csv')
View(dt)
dt = fread('/Users/codycooper/Documents/GitHub/dbt-example/data/pre-existing-conditions-by-congressional-district.xlsx - number with pre-ex by state.csv',
skip =2)
View(dt)
init_names = names(dt)
init_names %>% gsub('[[:alpha:]]','',.)
library(dplyr)
init_names %>% gsub('[[:alpha:]]','',.)
pre_exit = grep('Pre', names(dt), value = T)
pre_exit
pre_exit = grep('Age', names(dt), value = T)
pre_exit
pre_exit %>% gsub('[[:alpha:]]','',.) %>%
source('~/.active-rstudio-document', echo=TRUE)
pre_exit %>% gsub('[[:alpha:]]| - ','',.)
pre_exit %>% gsub('[[:alpha:]]|  - ','',.)
pre_exit %>% gsub('[[:alpha:]]|-','',.)
pre_exit %>% gsub('[[:alpha:]]|  - ','',.) %>%
gsub
pre_exit %>% gsub('[[:alpha:]]|  - ','',.)
pre_exit %>% gsub('[[:alpha:]]|  - $','',.)
dt = fread('/Users/codycooper/Documents/GitHub/dbt-example/data/pre-existing-conditions-by-congressional-district.xlsx - number with pre-ex by state.csv',
skip =2)
View(dt)
dt = fread('/Users/codycooper/Documents/GitHub/dbt-example/data/pre-existing-conditions-by-congressional-district.xlsx - number with pre-ex by state.csv')
View(dt)
library(data.table)
setwd("~/Documents/GitHub/tbd-insur-model")
dt = fread('data/travel_insurance.csv')
names(dt)
# look at ration of claims
dt[, table(claim)]
class(dt)
# look at ration of claims
dt[, table.prop(claim)]
# look at ration of claims
dt[, prop.table(claim)]
dt[, claim_bin := ifelse(claim == 'yes', 1.0)]
dt[, claim_bin := ifelse(claim == 'yes', 1,0)]
# look at ration of claims
dt[, prop.table(claim_bin)]
dt[, claim_bin := ifelse(claim == 'Yes', 1,0)]
# look at ration of claims
dt[, prop.table(claim_bin)]
# look at ration of claims
dt[, table(claim_bin)]
# look at ration of claims
dt[, table(claim_bin)/sum(table(claim_bin))]
# look at ration of claims
dt[, table(claim_bin)/sum(table(claim_bin))*100]
#Claims by region
ggplot(dt, aes(x=destination, y=claim_bin, fill=destination)) +
geom_bar(stat="identity")+theme_minimal()
library(ggplot)
library(ggplot2)
dt[,.(num_claims = sum(claim_bin)), by = destination]
#Claims by region
ggplot(dt[,.(num_claims = sum(claim_bin)), by = destination], aes(x=destination, y=claim_bin, fill=destination)) +
geom_bar(stat="identity")+theme_minimal()
#Claims by region
ggplot(dt[,.(num_claims = sum(claim_bin)), by = destination], aes(x=destination, y=num_claims, fill=destination)) +
geom_bar(stat="identity")+theme_minimal()
#Claims by region
ggplot(dt[,.(num_claims = sum(claim_bin)), by = destination], aes(x=destination, y=num_claims, fill=destination)) +
geom_bar(stat="identity")+theme_minimal()+theme(legend.position = "none")
#Claims by region
ggplot(dt[,.(num_claims = sum(claim_bin)), by = destination][num_claims > 0 ], aes(x=destination, y=num_claims, fill=destination)) +
geom_bar(stat="identity")+theme_minimal()+theme(legend.position = "none")
library(DT)
datatable(claims_dest)
#Claims by region
claims_dest = dt[,.(num_claims = sum(claim_bin)), by = destination][num_claims > 0 ]
datatable(claims_dest)
#Claims by region
claims_dest = dt[,.(
num_claims = sum(claim_bin),
claims_per_policy = sum(claim_bin) / .N
), by = destination][num_claims > 0 ]
#Singapore has a lot of claims!!
datatable(claims_dest)
datatable(claims_dest %>% setorder(num_claims))
datatable(claims_dest %>% setorder(-num_claims))
# Claims by Gender
ggplot(dt[,.(num_claims = sum(claim_bin)), by = gender], aes(x=gender, y=num_claims, fill=gender)) +
geom_bar(stat="identity")+theme_minimal()+theme(legend.position = "none")
unique(dt$gender)
ggplot(claims_dest, aes(x="", y=num_claims, fill=destination)) +
geom_bar(stat="identity", width=1) +
coord_polar("y", start=0)
ggplot(claims_dest, aes(x="", y=num_claims, fill=destination)) +
geom_bar(stat="identity", width=1) +
coord_polar("y", start=0)+theme(legend.position = "none")
dt[duration == 0, duration := mean(duration)]
hist(dt$duration)
hist(log(dt$duration))
